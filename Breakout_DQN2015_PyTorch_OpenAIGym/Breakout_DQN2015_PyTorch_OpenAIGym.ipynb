{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Breakout_DQN2015_PyTorch_OpenAIGym.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"IndCxwOo0exP","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [18/12/07] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","# 自作クラス\n","#from Agent import Agent\n","\n","class Academy( object ):\n","    \"\"\"\n","    エージェントの強化学習環境\n","    ・強化学習モデルにおける環境 Enviroment に対応\n","    ・学習や推論を行うための設定を行う。\n","    \n","    [public]\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _max_episode : <int> エピソードの最大回数。最大回数数に到達すると、Academy と全 Agent のエピソードを完了する。\n","        _max_time_step : <int> 時間ステップの最大回数\n","        _save_step : <int> 保存間隔（エピソード数）\n","        _agents : list<AgentBase>\n","        _done : <bool> エピソードが完了したかのフラグ\n","\n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__( self, max_episode = 1, max_time_step = 100, save_step = 5 ):\n","        self._max_episode = max_episode\n","        self._max_time_step = max_time_step\n","        self._save_step = save_step\n","        self._agents = []\n","        self._done = False\n","        return\n","\n","    def academy_reset( self ):\n","        \"\"\"\n","        学習環境をリセットする。\n","        \"\"\"\n","        if( self._agents != None ):\n","            for agent in self._agents:\n","                agent.agent_reset()        \n","\n","        self._done = False\n","        return\n","\n","    def done( self ):\n","        \"\"\"\n","        エピソードを完了にする。\n","        \"\"\"\n","        self._done = True\n","        return\n","\n","    def is_done( self ):\n","        \"\"\"\n","        Academy がエピソードを完了したかの取得\n","        \"\"\"\n","        return self._done\n","\n","    def add_agent( self, agent ):\n","        \"\"\"\n","        学習環境にエージェントを追加する。\n","        \"\"\"\n","        self._agents.append( agent )\n","        return\n","\n","\n","    def academy_run( self ):\n","        \"\"\"\n","        学習環境を実行する\n","        \"\"\"\n","        # エピソードを試行\n","        for episode in range( 0, self._max_episode ):\n","            print( \"現在のエピソード数：\", episode )\n","\n","            # 学習環境を RESET\n","            self.academy_reset()\n","\n","            # 時間ステップを 1ステップづつ進める\n","            for time_step in range( 0 ,self._max_time_step ):\n","                dones = []\n","\n","                # 学習環境の動画のフレームを追加\n","                if( episode % self._save_step == 0 ):\n","                    self.add_frame( episode, time_step )\n","                if( episode == self._max_episode - 1 ):\n","                    self.add_frame( episode, time_step )\n","\n","                for agent in self._agents:\n","                    done = agent.agent_step( episode, time_step )\n","                    dones.append( done )\n","\n","                # 全エージェントが完了した場合\n","                if( all(dones) == True ):\n","                    break\n","\n","            # Academy と全 Agents のエピソードを完了\n","            self._done = True\n","            for agent in self._agents:\n","                agent.agent_on_done( episode, time_step )\n","\n","            # 動画を保存\n","            if( episode % self._save_step == 0 ):\n","                self.save_frames( \"RL_ENV_Episode{}.gif\".format(episode) )\n","                self._frames = []\n","\n","            if( episode == self._max_episode - 1 ):\n","                self.save_frames( \"RL_ENV_Episode{}.gif\".format(episode) )\n","                self._frames = []\n","        \n","        return\n","\n","\n","    def add_frame( self, episode, times_step ):\n","        \"\"\"\n","        強化学習の環境の１フレームをリストに追加する\n","        \"\"\"\n","        #frame = None\n","        #self._frames.append( frame )\n","        return\n","\n","    def save_frames( self, file_name ):\n","        \"\"\"\n","        外部ファイルに動画を保存する。\n","        \"\"\"\n","        return\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OLvdfNKJ5NRt","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [19/03/18] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","import os.path\n","\n","#from JSAnimation.IPython_display import display_animation\n","#from matplotlib import animation\n","#from IPython.display import display\n","#%matplotlib inline\n","\n","# 自作クラス\n","#from Academy import Academy\n","#from Agent import Agent\n","\n","\n","class BreakoutAcademy( Academy ):\n","    \"\"\"\n","    エージェントの強化学習環境\n","    ・強化学習モデルにおける環境 Enviroment に対応\n","    ・学習や推論を行うための設定を行う。\n","    \n","    [public]\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _env : OpenAIGym の ENV\n","\n","        _frames : list<>\n","            動画のフレーム（１つの要素が１画像のフレーム）\n","        \n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__( self, env, max_episode = 1, max_time_step = 100, save_step = 100 ):\n","        super().__init__( max_episode, max_time_step, save_step )\n","        self._env = env\n","        self._frames = []\n","        return\n","\n","\n","    def academy_reset( self ):\n","        \"\"\"\n","        学習環境をリセットする。\n","        ・エピソードの開始時にコールされる\n","        \"\"\"\n","        if( self._agents != None ):\n","            for agent in self._agents:\n","                agent.agent_reset()        \n","\n","        self._done = False\n","        #self._env.reset()\n","        return\n","\n","    def academy_run( self ):\n","        \"\"\"\n","        学習環境を実行する\n","        \"\"\"\n","        #self.academy_reset()\n","\n","        # エピソードを試行\n","        for episode in range( 0, self._max_episode ):\n","            # 学習環境を RESET\n","            self.academy_reset()\n","\n","            # 時間ステップを 1ステップづつ進める\n","            for time_step in range( 0 ,self._max_time_step ):\n","                dones = []\n","\n","                if( episode % self._save_step == 0 ):\n","                    # 学習環境の動画のフレームを追加\n","                    self.add_frame( episode, time_step )\n","                if( episode == self._max_episode - 1 ):\n","                    # 学習環境の動画のフレームを追加\n","                    self.add_frame( episode, time_step )\n","\n","                for agent in self._agents:\n","                    done = agent.agent_step( episode, time_step )\n","                    dones.append( done )\n","\n","                # 全エージェントが完了した場合\n","                if( all(dones) == True ):\n","                    break\n","\n","            # Academy と全 Agents のエピソードを完了\n","            self._done = True\n","            for agent in self._agents:\n","                agent.agent_on_done( episode, time_step )\n","\n","            # 動画を保存\n","            if( episode % self._save_step == 0 ):\n","                self.save_frames( \"RL_ENV_{}_Episode{}.gif\".format(self._env.spec.id, episode) )\n","                self._frames = []\n","\n","            if( episode == self._max_episode - 1 ):\n","                self.save_frames( \"RL_ENV_{}_Episode{}.gif\".format(self._env.spec.id, episode) )\n","                self._frames = []\n","\n","        return\n","\n","\n","    def add_frame( self, episode, times_step ):\n","        \"\"\"\n","        強化学習環境の１フレームを追加する\n","        \"\"\"\n","        frame = self._env.render( mode='rgb_array' )\n","        self._frames.append( frame )\n","\n","        return\n","\n","\n","    def save_frames( self, file_name = \"RL_ENV_CartPole-v0.mp4\" ):\n","        \"\"\"\n","        外部ファイルに動画を保存する。\n","        \"\"\"\n","        plt.clf()\n","        plt.figure(\n","            figsize=( self._frames[0].shape[1]/72.0, self._frames[0].shape[0]/72.0 ),\n","            dpi=72\n","        )\n","        patch = plt.imshow( self._frames[0] )\n","        plt.axis('off')\n","        \n","        def animate(i):\n","            patch.set_data( self._frames[i] )\n","\n","        anim = animation.FuncAnimation(\n","                   plt.gcf(), \n","                   animate, \n","                   frames = len( self._frames ),\n","                   interval=50\n","        )\n","\n","        # 動画の保存\n","        ftitle, fext = os.path.splitext(file_name)\n","        if( fext == \".gif\" ):\n","            anim.save( file_name, writer = 'imagemagick' )\n","        else:\n","            anim.save( file_name )\n","\n","        #display( display_animation(anim, default_mode='loop') )\n","        \n","        plt.close()\n","\n","        return\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Kzgek4Gz5Z-R","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [19/03/18] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","import os.path\n","\n","# 自作クラス\n","#from Academy import Academy\n","#from Agent import Agent\n","\n","\n","class BreakoutAcademy( Academy ):\n","    \"\"\"\n","    エージェントの強化学習環境\n","    ・強化学習モデルにおける環境 Enviroment に対応\n","    ・学習や推論を行うための設定を行う。\n","    \n","    [public]\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _env : OpenAIGym の ENV\n","\n","        _frames : list<>\n","            動画のフレーム（１つの要素が１画像のフレーム）\n","        \n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__( self, env, max_episode = 1, max_time_step = 100, save_step = 100 ):\n","        super().__init__( max_episode, max_time_step, save_step )\n","        self._env = env\n","        self._frames = []\n","        return\n","\n","\n","    def academy_reset( self ):\n","        \"\"\"\n","        学習環境をリセットする。\n","        ・エピソードの開始時にコールされる\n","        \"\"\"\n","        if( self._agents != None ):\n","            for agent in self._agents:\n","                agent.agent_reset()        \n","\n","        self._done = False\n","        #self._env.reset()\n","        return\n","\n","    def academy_run( self ):\n","        \"\"\"\n","        学習環境を実行する\n","        \"\"\"\n","        #self.academy_reset()\n","\n","        # エピソードを試行\n","        for episode in range( 0, self._max_episode ):\n","            # 学習環境を RESET\n","            self.academy_reset()\n","\n","            # 時間ステップを 1ステップづつ進める\n","            for time_step in range( 0 ,self._max_time_step ):\n","                dones = []\n","\n","                if( episode % self._save_step == 0 ):\n","                    # 学習環境の動画のフレームを追加\n","                    self.add_frame( episode, time_step )\n","                if( episode == self._max_episode - 1 ):\n","                    # 学習環境の動画のフレームを追加\n","                    self.add_frame( episode, time_step )\n","\n","                for agent in self._agents:\n","                    done = agent.agent_step( episode, time_step )\n","                    dones.append( done )\n","\n","                # 全エージェントが完了した場合\n","                if( all(dones) == True ):\n","                    break\n","\n","            # Academy と全 Agents のエピソードを完了\n","            self._done = True\n","            for agent in self._agents:\n","                agent.agent_on_done( episode, time_step )\n","\n","            # 動画を保存\n","            if( episode % self._save_step == 0 ):\n","                self.save_frames( \"RL_ENV_{}_Episode{}.gif\".format(self._env.spec.id, episode) )\n","                self._frames = []\n","\n","            if( episode == self._max_episode - 1 ):\n","                self.save_frames( \"RL_ENV_{}_Episode{}.gif\".format(self._env.spec.id, episode) )\n","                self._frames = []\n","\n","        return\n","\n","\n","    def add_frame( self, episode, times_step ):\n","        \"\"\"\n","        強化学習環境の１フレームを追加する\n","        \"\"\"\n","        frame = self._env.render( mode='rgb_array' )\n","        self._frames.append( frame )\n","\n","        return\n","\n","\n","    def save_frames( self, file_name = \"RL_ENV_CartPole-v0.mp4\" ):\n","        \"\"\"\n","        外部ファイルに動画を保存する。\n","        \"\"\"\n","        plt.clf()\n","        plt.figure(\n","            figsize=( self._frames[0].shape[1]/72.0, self._frames[0].shape[0]/72.0 ),\n","            dpi=72\n","        )\n","        patch = plt.imshow( self._frames[0] )\n","        plt.axis('off')\n","        \n","        def animate(i):\n","            patch.set_data( self._frames[i] )\n","\n","        anim = animation.FuncAnimation(\n","                   plt.gcf(), \n","                   animate, \n","                   frames = len( self._frames ),\n","                   interval=50\n","        )\n","\n","        # 動画の保存\n","        ftitle, fext = os.path.splitext(file_name)\n","        if( fext == \".gif\" ):\n","            anim.save( file_name, writer = 'imagemagick' )\n","        else:\n","            anim.save( file_name )\n","\n","        plt.close()\n","\n","        return\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Rh5cSIpD3bqH","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [18/12/04] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","\n","\n","class Agent( object ):\n","    \"\"\"\n","    強化学習におけるエージェントをモデル化したクラス。\n","    ・実際の Agent クラスの実装は、このクラスを継承し、オーバーライドするを想定している。\n","\n","    [public]\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _brain : <Brain> エージェントの Brain への参照\n","        _observations : list<動的な型> エージェントが観測できる状態\n","        _total_reward : <float> 割引利得の総和\n","        _gamma : <float> 収益の割引率\n","        _done : <bool> エピソードの完了フラグ\n","        _state : <int> エージェントの現在の状態 s\n","        _action : <int> エピソードの現在の行動 a\n","        _s_a_historys : list< [int,int] > エピソードの状態と行動の履歴\n","        _reward_historys : list<float> 割引利得の履歴 / shape = [n_episode]\n","\n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__( \n","        self, \n","        brain = None, \n","        gamma = 0.9, \n","        state0 = 0\n","    ):\n","        self._brain = brain\n","        self._observations = []\n","        self._total_reward = 0.0\n","        self._gamma = gamma\n","        self._done = False\n","        self._state = state0\n","        self._action = np.nan\n","        self._s_a_historys = [ [ self._state, self._action ] ]\n","        self._reward_historys = [self._total_reward]\n","        return\n","\n","    def print( self, str ):\n","        print( \"----------------------------------\" )\n","        print( \"Agent\" )\n","        print( self )\n","        print( str )\n","\n","        print( \"_brain : \\n\", self._brain )\n","        print( \"_observations : \\n\", self._observations )\n","        print( \"_total_reward : \\n\", self._total_reward )\n","        print( \"_gamma : \\n\", self._gamma )\n","        print( \"_done : \\n\", self._done )\n","        print( \"_state : \\n\", self._state )\n","        print( \"_action : \\n\", self._action )\n","        print( \"_s_a_historys : \\n\", self._s_a_historys )\n","        print( \"_reward_historys : \\n\", self._reward_historys )\n","        print( \"----------------------------------\" )\n","        return\n","\n","    def get_s_a_historys( self ):\n","        return self._s_a_historys\n","\n","    def get_reward_historys( self ):\n","        return self._reward_historys\n","\n","    def collect_observations( self ):\n","        \"\"\"\n","        Agent が観測している State を Brain に提供する。\n","        ・Brain が、エージェントの状態を取得時にコールバックする。\n","        \"\"\"\n","        self._observations = []\n","        return self._observations\n","\n","\n","    def set_brain( self, brain ):\n","        \"\"\"\n","        エージェントの Brain を設定する。\n","        \"\"\"\n","        self._brain = brain\n","        return\n","\n","    def add_vector_obs( self, observation ):\n","        \"\"\"\n","        エージェントが観測できる状態を追加する。\n","        \"\"\"\n","        self._observations.append( observation )\n","        return\n","\n","    def done( self ):\n","        \"\"\"\n","        エピソードを完了にする。\n","        \"\"\"\n","        self._done = True\n","        return\n","\n","    def is_done( self ):\n","        \"\"\"\n","        Academy がエピソードを完了したかの取得\n","        \"\"\"\n","        return self._done\n","\n","    def set_total_reword( self, total_reward ):\n","        \"\"\"\n","        報酬をセットする\n","        \"\"\"\n","        self._total_reward = total_reward\n","        return self._total_reward\n","\n","    def add_reward( self, reward, time_step ):\n","        \"\"\"\n","        報酬を加算する\n","        ・割引収益 Rt = Σ_t γ^t r_t+1 になるように報酬を加算する。\n","        \"\"\"\n","        self._total_reward += (self._gamma**time_step) * reward\n","        return self._total_reward\n","\n","    def agent_reset( self ):\n","        \"\"\"\n","        エージェントの再初期化処理\n","        \"\"\"\n","        self._total_reward = 0.0\n","        self._done = False\n","        self._state = self._s_a_historys[0][0]\n","        self._action = self._s_a_historys[0][1]\n","        self._s_a_historys = [ [ self._state, self._action ] ]\n","        return\n","\n","    def agent_step( self, episode, time_step ):\n","        \"\"\"\n","        エージェント [Agent] の次の状態を決定する。\n","        ・Academy から各時間ステップ度にコールされるコールバック関数\n","\n","        [Args]\n","            episode : <int> 現在のエピソード数\n","            time_step : <int> 現在の時間ステップ\n","\n","        [Returns]\n","            done : <bool> エピソードの完了フラグ\n","        \"\"\"\n","        self._done = False\n","        return self._done\n","    \n","\n","    def agent_on_done( self, episode, time_step ):\n","        \"\"\"\n","        Academy のエピソード完了後にコールされ、エピソードの終了時の処理を記述する。\n","        ・Academy からコールされるコールバック関数\n","\n","        [Args]\n","            episode : <int> 現在のエピソード数\n","            time_step : <int> エピソード完了時の時間ステップ数\n","        \"\"\"\n","        return\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9SGPC5jS39TG","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [18/12/05] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","\n","class Brain( object ):\n","    \"\"\"\n","    エージェントの意思決定ロジック\n","    ・複数のエージェントが同じ意識決定ロジックを共有出来るように、Brain として class 化する。\n","    ・移動方法などの Action を設定する。\n","\n","    [public]\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _n_states : <int> 状態の要素数\n","        _n_actions : <int> 行動の要素数\n","                \n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        n_states,\n","        n_actions\n","    ):\n","        self._n_states = n_states\n","        self._n_actions = n_actions\n","        return\n","\n","    def print( self, str ):\n","        print( \"----------------------------------\" )\n","        print( \"Brain\" )\n","        print( self )\n","        print( str )\n","        print( \"_n_states : \\n\", self._n_states )\n","        print( \"_n_actions : \\n\", self._n_actions )\n","        print( \"----------------------------------\" )\n","        return\n","\n","    def reset_brain( self ):\n","        \"\"\"\n","        Brain を再初期化する\n","        \"\"\"        \n","        return\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N_tn9N1H5t2a","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [19/03/18] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","\n","# PyTorch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Flatten( nn.Module) :\n","    '''コンボリューション層の出力画像を1次元に変換する層を定義'''\n","\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","\n","class QNetworkCNN( nn.Module ):\n","    \"\"\"\n","    DQN のネットワーク構成\n","    PyTorch の nn.Module を継承して\n","\n","    [public]\n","\n","    \"\"\"\n","    def __init__( self, device, in_channles, n_actions ):\n","        \"\"\"\n","        [Args]\n","            _device : <torch.device> 実行デバイス\n","\n","            in_channles : チャンネル数（＝入力画像データの枚数）\n","            n_actions : 状態数 |A| / 出力ノード数に対応する。\n","        \"\"\"\n","        super( QNetworkCNN, self ).__init__()\n","        self._device = device\n","\n","        def init_wight( module ):\n","            \"\"\"\n","            ネットワーク層の重みが直交行列になるように初期化\n","            \"\"\"\n","            # ? gain 値を取得（Relu⇒√2）\n","            gain = nn.init.calculate_gain( \"relu\" )\n","\n","            #\n","            nn.init.orthogonal_( module.weight.data, gain = gain )\n","            nn.init.constant_( module.bias.data, 0 )\n","            return module\n","\n","        self.layer = nn.Sequential(\n","            init_wight( nn.Conv2d( in_channels = in_channles, out_channels = 32, kernel_size = 8, stride = 4 ) ),\n","            nn.ReLU(),\n","            init_wight( nn.Conv2d( in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2 ) ),\n","            nn.ReLU(),\n","            init_wight( nn.Conv2d( in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1 ) ),\n","            nn.ReLU(),\n","            Flatten(),\n","            init_wight( nn.Linear( in_features = 7*7*64, out_features = 512 ) ),\n","            nn.ReLU(),\n","            init_wight( nn.Linear( in_features = 512, out_features = n_actions ) )\n","        )\n","\n","        return\n","\n","    def forward( self, x ):\n","        \"\"\"\n","        ネットワークの順方向での更新処理\n","        \"\"\"\n","        # 画像のピクセル値0-255を0-1に正規化する\n","        #x = x / 255.0\n","\n","        output = self.layer(x)\n","\n","        return output\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Oqfwq6U85w0U","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [19/03/19] : 新規作成\n","                ・参考：https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","from collections import deque\n","\n","# OpenAI Gym\n","import gym\n","from gym import spaces\n","from gym.spaces.box import Box\n","\n","# PyTorch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# OpenCV\n","import cv2\n","cv2.ocl.setUseOpenCL(False)     # ?\n","\n","\n","class NoopResetEnv( gym.Wrapper ):\n","    \"\"\"\n","    強化学習環境のリセット直後の特定の開始状態で大きく依存して学習が進むのを防ぐために、\n","    強化学習環境のリセット直後、数ステップ間は何も学習しないプロセスを実施する。\n","    ・OpenAI Gym の env をラッピングして実装している。\n","    ・atari_wrappers.py と同じ内容\n","\n","    [public]\n","        env : OpenAIGym の ENV\n","        noop_max : <int> 何も学習しないステップ数\n","        override_num_noops : \n","        noop_action : <int> 何もしない行動の値\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","\n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__(self, env, noop_max=30):\n","        gym.Wrapper.__init__(self, env)\n","        self.noop_max = noop_max\n","        self.override_num_noops = None\n","        self.noop_action = 0\n","        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        reset() メソッドをオーバーライド\n","        Do no-op action for a number of steps in [1, noop_max].\n","        \"\"\"\n","        self.env.reset(**kwargs)\n","        if self.override_num_noops is not None:\n","            noops = self.override_num_noops\n","        else:\n","            noops = self.unwrapped.np_random.randint(\n","                1, self.noop_max + 1)  # pylint: disable=E1101\n","        assert noops > 0\n","        obs = None\n","        for _ in range(noops):\n","            obs, _, done, _ = self.env.step(self.noop_action)\n","            if done:\n","                obs = self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, ac):\n","        \"\"\"\n","        step() メソッドをオーバーライト\n","        \"\"\"\n","        return self.env.step(ac)\n","\n","\n","class EpisodicLifeEnv(gym.Wrapper):\n","    \"\"\"\n","    Breakout は５機のライフがあるので、５回失敗でゲーム終了となるが、\n","    この残機では学習が面倒なので、１回失敗でゲーム終了に設定する。\n","    但し、１回失敗毎に完全にリセットすると、初期状態ばかり学習してしまい、過学習してしまいやすいので、\n","    １回失敗でのリセットでは、崩したブロックの状態はそのままにしておいて、\n","    ５回失敗でのリセットでは、崩したブロックの状態もリセットする完全なリセットとする。\n","    ・OpenAI Gym の env をラッピングして実装している。\n","    ・atari_wrappers.py と同じ内容？\n","\n","    [public]\n","        env : OpenAIGym の ENV\n","        lives : <int> 残機\n","        was_real_done : <bool> 完全なリセットの意味での終了フラグ\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","\n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__(self, env):\n","        gym.Wrapper.__init__(self, env)\n","        self.lives = 0\n","        self.was_real_done = True\n","\n","    def step(self, action):\n","        \"\"\"\n","        step() メソッドをオーバーライト\n","        \"\"\"\n","        obs, reward, done, info = self.env.step(action)\n","        self.was_real_done = done\n","        # check current lives, make loss of life terminal,\n","        # then update lives to handle bonus lives\n","        lives = self.env.unwrapped.ale.lives()\n","        if lives < self.lives and lives > 0:\n","            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n","            # so its important to keep lives > 0, so that we only reset once\n","            # the environment advertises done.\n","            done = True\n","        self.lives = lives\n","        return obs, reward, done, info\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        reset() メソッドをオーバーライト\n","        ・5機とも失敗したら、本当にリセット\n","        \"\"\"\n","        if self.was_real_done:\n","            obs = self.env.reset(**kwargs)\n","        else:\n","            # no-op step to advance from terminal/lost life state\n","            obs, _, _, _ = self.env.step(0)\n","        self.lives = self.env.unwrapped.ale.lives()\n","        return obs\n","\n","\n","class MaxAndSkipEnv(gym.Wrapper):\n","    \"\"\"\n","    Breakout は 60FPS で動作するが、この速さで動かすと早すぎるので、\n","    ４フレーム単位で行動を判断させ、４フレーム連続で同じ行動を行うようにする。\n","    これにより、60FPS → 15 FPS となる。\n","    但し、atari のゲームには、奇数フレームと偶数フレームで現れる画像が異なるゲームがあるために、\n","    画面上のチラツキを抑える意味で、最後の3、4フレームの最大値をとった画像を observation として採用する。\n","    ・OpenAI Gym の env をラッピングして実装している。\n","    ・atari_wrappers.py とほぼ同じ内容\n","\n","    [public]\n","        env : OpenAIGym の ENV\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _obs_buffer :\n","        _skip : <int> フレームのスキップ数\n","\n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__(self, env, skip=4):\n","        gym.Wrapper.__init__(self, env)\n","        # most recent raw observations (for max pooling across time steps)\n","        self._obs_buffer = np.zeros(\n","            (2,)+env.observation_space.shape, dtype=np.uint8)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        \"\"\"\n","        step() メソッドをオーバーライト\n","        Repeat action, sum reward, and max over last observations.\n","        \"\"\"\n","        total_reward = 0.0\n","        done = None\n","        for i in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            if i == self._skip - 2:\n","                self._obs_buffer[0] = obs\n","            if i == self._skip - 1:\n","                self._obs_buffer[1] = obs\n","            total_reward += reward\n","            if done:\n","                break\n","        # Note that the observation on the done=True frame\n","        # doesn't matter\n","        max_frame = self._obs_buffer.max(axis=0)\n","\n","        return max_frame, total_reward, done, info\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        reset() メソッドをオーバーライト\n","        \"\"\"\n","        return self.env.reset(**kwargs)\n","\n","\n","class ClipRewardEnv(gym.RewardWrapper):\n","    \"\"\"\n","    報酬のクリッピング\n","    \"\"\"\n","    def _reward(self, reward):\n","        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n","        return np.sign(reward)\n","\n","      \n","\n","class WarpFrame(gym.ObservationWrapper):\n","    \"\"\"\n","    画像サイズをNatureのDQN論文と同じ84x84のグレースケールに reshape する。\n","\n","    [public]\n","        env : OpenAIGym の ENV\n","        observation_space : <spaces.Box> obsevation の shape / 継承先の gym.ObservationWrapper からの変数\n","        width : <int> 入力画像の幅のリサイズ後の値\n","        height : <int> 入力画像の高さのリサイズ後の値\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","\n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__(self, env):\n","        gym.ObservationWrapper.__init__(self, env)\n","        self.width = 84\n","        self.height = 84\n","        self.observation_space = spaces.Box(\n","            low=0, high=255,\n","            shape=(self.height, self.width, 1),\n","            dtype=np.uint8\n","        )\n","\n","    def observation(self, frame):\n","        \"\"\"\n","        observation の Getter をオーバーライド\n","        \"\"\"\n","        # 入力画像をグレースケールに変換\n","        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n","\n","        #\n","        frame = cv2.resize(\n","            frame, (self.width, self.height),\n","            interpolation=cv2.INTER_AREA\n","        )\n","\n","        return frame    # [width,height] で return\n","\n","      \n","\n","class WrapFrameStack(gym.Wrapper):\n","    def __init__(self, env, n_stack_frames = 4 ):\n","        \"\"\"\n","        obsevation を４フレーム分重ねる。\n","        モデルに一度に入力する画像データのフレーム数\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self.n_stack_frames = n_stack_frames\n","        self.frames = deque( [], maxlen = n_stack_frames )  # deque 構造で４フレーム分だけ確保\n","        obs_shape = env.observation_space.shape\n","        self.observation_space = spaces.Box( \n","            low = 0, high = 255, \n","            shape = ( obs_shape[0], obs_shape[1], obs_shape[2] * n_stack_frames )\n","        )\n","        return\n","\n","\n","    def reset(self):\n","        obs = self.env.reset()\n","        for _ in range(self.n_stack_frames):\n","            self.frames.append( obs )\n","\n","        # list 部分を numpy 化\n","        observation = np.array( self.frames )\n","        return observation\n","\n","    def step(self, action):\n","        \"\"\"\n","        step() メソッドをオーバーライト\n","        \"\"\"\n","        obs, reward, done, info = self.env.step(action)\n","        self.frames.append(obs)\n","\n","        # list 部分を numpy 化\n","        observation = np.array( self.frames )\n","        return observation, reward, done, info\n","\n","\n","class ScaledFloatFrame(gym.ObservationWrapper):\n","    def _observation(self, observation):\n","        # careful! This undoes the memory optimization, use\n","        # with smaller replay buffers only.\n","        return np.array(observation).astype(np.float32) / 255.0\n","      \n","      \n","class WrapMiniBatch(gym.ObservationWrapper):\n","    \"\"\"\n","    obsevation を ミニバッチ学習用のインデックス順に reshape する。\n","    [height, width, n_channels(=n_skip_frames)] → [n_channels(=n_skip_frames), height, width] \n","    \"\"\"\n","    def __init__(self, env=None):\n","        super(WrapMiniBatch, self).__init__(env)\n","        obs_shape = self.observation_space.shape\n","        self.observation_space = Box(\n","            self.observation_space.low[0, 0, 0],\n","            self.observation_space.high[0, 0, 0],\n","            shape = [obs_shape[2], obs_shape[0], obs_shape[1] ],\n","            dtype=self.observation_space.dtype\n","        )\n","        return\n","\n","    def observation(self, observation):\n","        \"\"\"\n","        observation の Getter をオーバーライド\n","        \"\"\"\n","        # [height, width, n_channels(=n_skip_frames)] → [n_channels(=n_skip_frames), height, width] に reshape\n","        return observation.transpose(2, 0, 1)\n","\n","\n","def make_env( device, env_id, seed = 8, n_noop_max = 30, n_skip_frame = 4, n_stack_frames = 4 ):\n","    \"\"\"\n","    上記 Wrapper による強化学習環境の生成メソッド\n","\n","    [Args]\n","        device : <torch.device> 実行デバイス\n","        env_id : \n","    \"\"\"\n","    # OpenAI Gym の強化学習環境生成\n","    env = gym.make(env_id)\n","\n","    # env をラッピングしていくことで、独自の設定を適用する。\n","    env = NoopResetEnv( env, noop_max = n_noop_max )\n","    env = MaxAndSkipEnv( env, skip = n_skip_frame )\n","    env.seed(seed)                  # 乱数シードの設定\n","    env = EpisodicLifeEnv(env)\n","    env = WarpFrame(env)\n","    env = ScaledFloatFrame(env)\n","    env = ClipRewardEnv(env)\n","    env = WrapFrameStack(env, n_stack_frames )\n","    #env = WrapMiniBatch(env)\n","    \n","    return env\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wLAzHTtW6gUp","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [19/02/12] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","import random\n","\n","# PyTorch\n","import torch\n","\n","from collections import namedtuple\n","\n","Transition = namedtuple(\n","    typename = \"Transition\",\n","    field_names = ( \"state\", \"action\", \"next_state\", \"reward\" )\n",")\n","\n","\n","class ExperienceReplay( object ):\n","    \"\"\"\n","    Experience Replay による学習用のミニバッチデータの生成を行うクラス\n","\n","    [public]\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _device : <torch.device> 実行デバイス\n","\n","        _capacity : [int] メモリの最大値\n","        _memory : [list] (s,a,s',a',r) のリスト（学習用データ）\n","        _index : [int] 現在のメモリのインデックス\n","             \n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        device,\n","        capacity = 10000\n","    ):\n","        self._device = device\n","        self._capacity = capacity\n","        self._memory = []\n","        self._index = 0\n","        return\n","\n","    def __len__( self ):\n","        return len( self._memory )\n","\n","    def print( self, str ):\n","        print( \"----------------------------------\" )\n","        print( \"CartPoleAgent\" )\n","        print( self )\n","        print( str )\n","        print( \"_device :\", self._device )\n","        print( \"_capacity :\", self._capacity )\n","        print( \"_memory : \\n\", self._memory )\n","        print( \"_index : \\n\", self._index )\n","        return\n","\n","    def push( self, state, action, next_state, reward ):\n","        \"\"\"\n","        学習用のデータのメモリに、データを push する\n","        [Args]\n","            state : <> 現在の状態 s\n","            action : <> 現在の行動 a\n","            next_state : <> 次の状態 s'\n","            reword : <> 報酬        \n","        [Returns]\n","        \"\"\"\n","        # 現在のメモリサイズが上限値以下なら、新たに容量を確保する。\n","        if( len(self._memory) < self._capacity ):\n","            self._memory.append( None )\n","\n","        # nametuple を使用して、メモリに値を格納\n","        self._memory[ self._index ] = Transition( state, action, next_state, reward )\n","\n","        # 現在のインデックスをづらす\n","        self._index = ( self._index + 1 ) % self._capacity\n","\n","        return\n","\n","    def pop( self, batch_size ):\n","        \"\"\"\n","        ミニバッチサイズ分だけ、ランダムにメモリの内容を取り出す\n","        [Args]\n","        [Returns]\n","            \n","        \"\"\"\n","        return random.sample( self._memory, batch_size )\n","\n","\n","    def get_mini_batch( self, batch_size ):\n","        \"\"\"\n","        ミニバッチデータを取得する\n","        [Args]\n","        [Returns]\n","        \"\"\"\n","        #----------------------------------------------------------------------\n","        # Experience Replay に基づき、ミニバッチ処理用のデータセットを生成する。\n","        #----------------------------------------------------------------------\n","        # メモリサイズがまだミニバッチサイズより小さい場合は、処理を行わない\n","        if( len(self._memory) < batch_size ):\n","            return None, None, None, None, None\n","\n","        # ミニバッチサイズ以上ならば、学習用データを pop する\n","        transitions = self.pop( batch_size )\n","        #print( \"transitions :\", transitions )\n","\n","        # 取り出したデータをミニバッチ学習用に reshape\n","        # transtions : shape = 1 step 毎の (s,a,s',r) * batch_size / shape = 32 * 4\n","        # → shape = (s * batch_size, a * batch_size, s' * batch_size, r * batch_size) / shape = 4 * 32\n","        batch = Transition( *zip(*transitions) )\n","        #print( \"batch :\", batch )\n","\n","        #\n","        state_batch = torch.cat( batch.state ).to(self._device)\n","        action_batch = torch.cat( batch.action ).to(self._device)\n","        reward_batch = torch.cat( batch.reward ).to(self._device)\n","\n","        non_final_next_states = torch.cat(\n","            [s for s in batch.next_state if s is not None]\n","        ).to(self._device)\n","\n","        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ncna7_wa4stU","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [19/03/18] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","\n","# OpenAI Gym\n","import gym\n","\n","# PyTorch\n","import torch\n","\n","# 自作クラス\n","#from Agent import Agent\n","\n","\n","class BreakoutAgent( Agent ):\n","    \"\"\"\n","    OpenAIGym の Breakout のエージェント\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _device : <torch.device> 実行デバイス\n","\n","        _env : OpenAI Gym の ENV\n","        _losses : list<float> 損失関数の値のリスト（長さはエピソード長）\n","\n","    \"\"\"\n","    def __init__( \n","        self,\n","        device,\n","        env,\n","        brain = None, \n","        gamma = 0.9\n","    ):\n","        super().__init__( brain, gamma, 0 )\n","        self._device = device\n","        self._env = env        \n","        self._total_reward = torch.FloatTensor( [0.0] )\n","        self._loss_historys = []\n","\n","        obs_shape = self._env.observation_space.shape  # (1, 84, 84)\n","\n","        # shape = [mini_batch, n_stack_frames(=n_channels), height, width]\n","        #self._observations = torch.zeros( 1, 4, obs_shape[1], obs_shape[2] )\n","        self._observations = torch.zeros( 1, 1, obs_shape[1], obs_shape[2] ).to(self._device)\n","        return\n","\n","    def print( self, str ):\n","        print( \"----------------------------------\" )\n","        print( \"BreakoutAgent\" )\n","        print( self )\n","        print( str )\n","        print( \"_device :\", self._device )\n","        print( \"_env :\", self._env )\n","        print( \"_brain : \\n\", self._brain )\n","        print( \"_observations : \\n\", self._observations )\n","        print( \"_total_reward : \\n\", self._total_reward )\n","        print( \"_gamma : \\n\", self._gamma )\n","        print( \"_done : \\n\", self._done )\n","        print( \"_s_a_historys : \\n\", self._s_a_historys )\n","        print( \"_reward_historys : \\n\", self._reward_historys )\n","        print( \"----------------------------------\" )\n","        return\n","\n","    def get_loss_historys( self ):\n","        return self._loss_historys\n","\n","    def agent_reset( self ):\n","        \"\"\"\n","        エージェントの再初期化処理\n","        \"\"\"\n","        observations_next = self._env.reset()   # shape = [1,84,84]\n","\n","        # numpy → Tensor に型変換\n","        observations_next = torch.from_numpy(observations_next).float().to(self._device)\n","\n","        # ミニバッチ用の次元を追加\n","        observations_next = torch.unsqueeze( observations_next, dim = 0 ).to(self._device)\n","\n","        #self._observations[:-1] = self._observations[1:]  # 0～2番目に1～3番目を上書き\n","        #self._observations[-1:] = observations_next  # 4番目に最新のobsを格納\n","        self._observations = observations_next\n","\n","        self._total_reward = torch.FloatTensor( [0.0] )\n","        self._done = False\n","        return\n","\n","    def agent_step( self, episode, time_step ):\n","        \"\"\"\n","        エージェント [Agent] の次の状態を決定する。\n","        ・Academy から各時間ステップ度にコールされるコールバック関数\n","\n","        [Args]\n","            episode : 現在のエピソード数\n","            time_step : 現在の時間ステップ\n","\n","        [Returns]\n","            done : bool\n","                   エピソードの完了フラグ\n","        \"\"\"\n","        # 既にエピソードが完了状態なら、そのまま return して、全エージェントの完了を待つ\n","        if( self._done == True):\n","            return self._done\n","\n","       \n","        #-------------------------------------------------------------------\n","        # ε-greedy 法の ε 値を減衰させる。\n","        #-------------------------------------------------------------------\n","        #self._brain.decay_epsilon()\n","        self._brain.decay_epsilon_episode( episode )\n","        \n","        #-------------------------------------------------------------------\n","        # 行動 a_t を求める\n","        #-------------------------------------------------------------------\n","        action = self._brain.action( self._observations, time_step )\n","        #print( \"action :\", action )\n","\n","        #-------------------------------------------------------------------\n","        # 行動を実行する。\n","        #-------------------------------------------------------------------\n","        observations_next, reward, env_done, info = self._env.step( action.item() )\n","\n","        # numpy → PyTorch 用の型に変換\n","        observations_next = torch.from_numpy(observations_next).float().to(self._device)\n","\n","        # ミニバッチ学習用の次元を追加\n","        observations_next = torch.unsqueeze( observations_next, dim = 0 ).to(self._device)\n","\n","        #print( \"reward :\", reward )\n","        #print( \"env_done :\", env_done )\n","        #print( \"info :\", info )\n","\n","        #------------------------------------------------------------------\n","        # 行動の実行により、次の時間での状態 s_{t+1} 報酬 r_{t+1} を求める。\n","        #------------------------------------------------------------------\n","        self.add_reward( reward, time_step )\n","        reward = torch.FloatTensor( [reward] ).to(self._device)\n","\n","        # env_done : ステップ数が最大数経過 OR 一定角度以上傾くと ⇒ True\n","        if( env_done == True ):\n","            # 次の状態は存在しない（＝終端状態）ので、None に設定する\n","            observations_next = None\n","        else:\n","            pass\n","\n","        #----------------------------------------\n","        # 価値関数の更新\n","        #----------------------------------------\n","        self._brain.update_q_function( self._observations, action, observations_next, reward )\n","\n","        #----------------------------------------\n","        # 状態の更新\n","        #----------------------------------------\n","        #self._observations[:-1] = self._observations[1:]  # 0～2番目に1～3番目を上書き\n","        #self._observations[-1:] = observations_next  # 4番目に最新のobsを格納\n","        self._observations = observations_next\n","\n","        #----------------------------------------\n","        # 完了時の処理\n","        #----------------------------------------\n","        if( env_done == True ):\n","            self.done()\n","\n","        return self._done\n","\n","\n","    def agent_on_done( self, episode, time_step ):\n","        \"\"\"\n","        Academy のエピソード完了後にコールされ、エピソードの終了時の処理を記述する。\n","        ・Academy からコールされるコールバック関数\n","\n","        [Args]\n","            episode : <int> 現在のエピソード数\n","        \"\"\"\n","        print( \"エピソード = {0} / 最終時間ステップ数 = {1}\".format( episode, time_step )  )\n","\n","        # 利得の履歴に追加\n","        self._reward_historys.append( self._total_reward )\n","\n","        # 損失関数の履歴に追加\n","        print( \"loss = %0.6f\" % self._brain.get_loss() )\n","        self._loss_historys.append( self._brain.get_loss() )\n","\n","        # 一定間隔で、Target Network と Main Network を同期する\n","        if( (episode % 2) == 0 ):\n","            self._brain.update_target_q_function()\n","\n","        return\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cpbCkc7b57vv","colab_type":"code","colab":{}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","\"\"\"\n","    更新情報\n","    [19/03/18] : 新規作成\n","    [xx/xx/xx] : \n","\"\"\"\n","import numpy as np\n","import random\n","\n","# 自作クラス\n","#from Brain import Brain\n","#from Agent import Agent\n","#from ExperienceReplay import ExperienceReplay\n","#from QNetworkCNN import QNetworkCNN\n","\n","# PyTorch\n","import torch\n","from torch  import nn   # ネットワークの構成関連\n","from torch import optim\n","import torch.nn.functional as F\n","\n","\n","class BreakoutDQN2015Brain( Brain ):\n","    \"\"\"\n","    Breakoutの Brain。\n","    ・DQN によるアルゴリズム\n","    \n","    [public]\n","\n","    [protected] 変数名の前にアンダースコア _ を付ける\n","        _device : <torch.device> 実行デバイス\n","\n","        _epsilon : <float> ε-greedy 法の ε 値\n","        _gamma : <float> 割引利得の γ 値\n","        _learning_rate : <float> 学習率\n","\n","        _q_function : <> 教師信号である古いパラメーター θ- で固定化された行動状態関数 Q(s,a,θ-)\n","        _expected_q_function : <> 推定行動状態関数 Q(s,a,θ)\n","        _memory : <ExperienceRelay> ExperienceRelayに基づく学習用のデータセット\n","\n","        _main_network : <QNetwork> DQNのネットワーク\n","        _target_network : <QNetwork> DQNのターゲットネットワーク\n","\n","        _loss_fn : <torch.> モデルの損失関数\n","        _optimizer : <torch.optimizer> モデルの最適化アルゴリズム\n","\n","        _n_stack_frames : <int> モデルに一度に入力する画像のフレーム数\n","\n","    [private] 変数名の前にダブルアンダースコア __ を付ける（Pythonルール）\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        device,\n","        n_states, n_actions,\n","        epsilon_init = 1.0, epsilon_final = 0.1, n_epsilon_step = 1000000,\n","        gamma = 0.9, learning_rate = 0.0001,\n","        batch_size = 32,\n","        memory_capacity = 10000,\n","        n_stack_frames = 4,\n","        n_skip_frames = 4\n","    ):\n","        super().__init__( n_states, n_actions )\n","        self._device = device\n","        self._epsilon = epsilon_init\n","        self._epsilon_init = epsilon_init\n","        self._epsilon_final = epsilon_final\n","        self._gamma = gamma\n","        self._learning_rate = learning_rate\n","        self._batch_size = batch_size\n","        self._n_stack_frames = n_stack_frames\n","        self._n_skip_frames = n_skip_frames\n","\n","        self._main_network = None\n","        self._target_network = None\n","        self.model()\n","\n","        self._loss_fn = None\n","        self._optimizer = None\n","\n","        self._q_function = None\n","        self._expected_q_function = None\n","        self._memory = ExperienceReplay( device = device, capacity = memory_capacity )\n","\n","        self._b_loss_init = False\n","\n","        self._epsilon_step = ( epsilon_init - epsilon_final ) / n_epsilon_step\n","        self._action_repeat = torch.LongTensor( [0] )\n","\n","        return\n","\n","    def print( self, str ):\n","        print( \"----------------------------------\" )\n","        print( \"BreakoutDQN2015Brain\" )\n","        print( self )\n","        print( str )\n","        print( \"_device :\", self._device )\n","        print( \"_n_states : \", self._n_states )\n","        print( \"_n_actions : \", self._n_actions )\n","        print( \"_epsilon : \", self._epsilon )\n","        print( \"_epsilon_init : \", self._epsilon_init )\n","        print( \"_epsilon_final : \", self._epsilon_final )\n","        print( \"_epsilon_step : \", self._epsilon_step )\n","        print( \"_gamma : \", self._gamma )\n","        print( \"_learning_rate : \", self._learning_rate )\n","        print( \"_batch_size : \", self._batch_size )\n","        print( \"_n_stack_frames : \", self._n_stack_frames )\n","        print( \"_n_skip_frames : \", self._n_skip_frames )\n","\n","        print( \"_q_function : \\n\", self._q_function )\n","        print( \"_expected_q_function : \\n\", self._expected_q_function )\n","        print( \"_memory :\", self._memory )\n","\n","        print( \"_main_network :\\n\", self._main_network )\n","        print( \"_target_network :\", self._target_network )\n","        print( \"_loss_fn :\\n\", self._loss_fn )\n","        print( \"_optimizer :\\n\", self._optimizer )\n","\n","        print( \"----------------------------------\" )\n","        return\n","\n","    def get_q_function( self ):\n","        \"\"\"\n","        Q 関数の値を取得する。\n","        \"\"\"\n","        return self._q_function\n","\n","\n","    def model( self ):\n","        \"\"\"\n","        DQN のネットワーク構成を構築する。\n","        \n","        [Args]\n","        [Returns]\n","        \"\"\"\n","        #------------------------------------------------\n","        # ネットワーク構成\n","        #------------------------------------------------        \n","        self._main_network = QNetworkCNN(\n","            device = self._device,\n","            in_channles = self._n_stack_frames,\n","            n_actions = self._n_actions\n","        ).to(self._device)\n","\n","        self._target_network = QNetworkCNN(\n","            device = self._device,\n","            in_channles = self._n_stack_frames,\n","            n_actions = self._n_actions\n","        ).to(self._device)\n","        \n","        print( \"main network :\", self._main_network )\n","        print( \"target network :\", self._target_network )\n","        return\n","\n","    def loss( self ):\n","        \"\"\"\n","        モデルの損失関数を設定する。\n","        [Args]\n","        [Returns]\n","            self._loss_fn : <> モデルの損失関数\n","        \"\"\"\n","        # smooth L1 関数（＝Huber 関数）\n","        self._loss_fn = F.smooth_l1_loss( \n","            input = self._q_function,                        # 行動価値関数 Q(s,a;θ) / shape = n_batch\n","            target = self._expected_q_function.unsqueeze(1)  # 推定行動価値関数 Q(s,a;θ)\n","        )\n","\n","        #print( \"loss_fn :\", self._loss_fn )\n","\n","        # loss 値の初回計算フラグ\n","        self._b_loss_init = True\n","\n","        return self._loss_fn\n","\n","    def get_loss( self ):\n","        if( self._b_loss_init == True ):\n","            return self._loss_fn.data\n","        else:\n","            return 0.0\n","\n","    def optimizer( self ):\n","        \"\"\"\n","        モデルの最適化アルゴリズムを設定する。\n","        [Args]\n","        [Returns]\n","            self._optimizer : <torch.optimizer> モデルの最適化アルゴリズム            \n","        \"\"\"\n","        # 最適化アルゴリズムとして、RMSprop を採用\n","        self._optimizer = optim.RMSprop(\n","            params = self._main_network.parameters(), \n","            lr = self._learning_rate \n","        )\n","\n","        return self._optimizer\n","\n","\n","    def predict( self, batch, state_batch, action_batch, reward_batch, non_final_next_states ):\n","        \"\"\"\n","        教師信号となる行動価値関数を求める\n","\n","        [Args]\n","        [Returns]\n","        \"\"\"\n","        #--------------------------------------------------------------------\n","        # ネットワークを推論モードへ切り替え（PyTorch特有の処理）\n","        #--------------------------------------------------------------------\n","        self._main_network.eval()\n","        self._target_network.eval()\n","\n","        #--------------------------------------------------------------------\n","        # 構築したDQNのネットワークが出力する Q(s,a) を求める。\n","        # 学習用データをモデルに流し込む\n","        # model(引数) で呼び出せるのは、__call__ をオーバライトしているため\n","        #--------------------------------------------------------------------\n","        # outputs / shape = batch_size * _n_actions\n","        outputs = self._main_network( state_batch ).to(self._device)\n","        #print( \"outputs :\", outputs )\n","\n","        # outputs から実際にエージェントが選択した action を取り出す\n","        # gather(...) : \n","        # dim = 1 : 列方向\n","        # index = action_batch : エージェントが実際に選択した行動は action_batch \n","        self._q_function = outputs.gather( 1, action_batch ).to(self._device)\n","        #print( \"_q_function :\", self._q_function )\n","\n","        #--------------------------------------------------------------------\n","        # 次の状態を求める\n","        #--------------------------------------------------------------------\n","        # 全部 0 で初期化\n","        next_state_values = torch.zeros( self._batch_size ).to(self._device)\n","\n","        # エージェントが done ではなく、next_state が存在するインデックス用のマスク\n","        non_final_mask = torch.ByteTensor(\n","            tuple( map(lambda s: s is not None,batch.next_state) )\n","        ).to(self._device)\n","        #print( \"non_final_mask :\", non_final_mask )\n","\n","        # Main Network ではなく Target Network からの出力\n","        next_outputs = self._target_network( non_final_next_states ).to(self._device)\n","        #print( \"next_outputs :\", next_outputs )\n","\n","        # detach() : ネットワークの出力の値を取り出す。Variable の誤差逆伝搬による値の更新が止まる？\n","        # 教師信号は固定された値である必要があるので、detach() で値が変更させないようにする。\n","        next_state_values[non_final_mask] = next_outputs.max(1)[0].detach().to(self._device)\n","        #print( \"next_state_values :\", next_state_values )\n","\n","        #--------------------------------------------------------------------\n","        # ネットワークの出力となる推定行動価値関数を求める\n","        #--------------------------------------------------------------------\n","        self._expected_q_function = reward_batch + self._gamma * next_state_values\n","\n","        return\n","\n","    def fit( self ):\n","        \"\"\"\n","        モデルを学習し、\n","        [Args]\n","        [Returns]\n","        \"\"\"\n","        # モデルを学習モードに切り替える。\n","        self._main_network.train()\n","\n","        # 損失関数を計算する\n","        self.loss()\n","\n","        # 勾配を 0 に初期化（この初期化処理が必要なのは、勾配がイテレーション毎に加算される仕様のため）\n","        self._optimizer.zero_grad()\n","\n","        # 誤差逆伝搬\n","        self._loss_fn.backward()\n","\n","        # backward() で計算した勾配を元に、設定した optimizer に従って、重みを更新\n","        self._optimizer.step()\n","\n","        #print( \"loss :\", self._loss_fn.data )\n","\n","        return\n","\n","    def decay_epsilon( self ):\n","        \"\"\"\n","        ε-greedy 法の ε 値を減衰させる。\n","        \"\"\"\n","        if( self._epsilon > self._epsilon_final and self._epsilon <= self._epsilon_init ):\n","            self._epsilon -= self._epsilon_step\n","\n","        return\n","      \n","    def decay_epsilon_episode( self, episode ):\n","        self._epsilon = 0.5 * ( 1 / (episode + 1) )\n","        return\n","\n","      \n","    def action( self, state, time_step ):\n","        \"\"\"\n","        Brain のロジックに従って、現在の状態 s での行動 a を決定する。\n","        ・ε-グリーディー法に従った行動選択\n","        [Args]\n","            state : int\n","                現在の状態\n","        \"\"\"\n","        # フレームスキップ間は、同じ行動をする。\n","        if( (time_step % self._n_skip_frames) != 0 ):\n","            action = self._action_repeat\n","        else:\n","            # ε-グリーディー法に従った行動選択\n","            if( self._epsilon <= np.random.uniform(0,1) ):\n","                #------------------------------\n","                # Q の最大化する行動を選択\n","                #------------------------------\n","                # model を推論モードに切り替える（PyTorch特有の処理）\n","                self._main_network.eval()\n","\n","                # 微分を行わない処理の範囲を with 構文で囲む\n","                with torch.no_grad():\n","                    # テストデータをモデルに流し込み、モデルの出力を取得する\n","                    outputs = self._main_network( state ).to(self._device)\n","                    #print( \"outputs :\", outputs )\n","                    #print( \"outputs.data :\", outputs.data )\n","\n","                    # dim = 1 ⇒ 列方向で最大値をとる\n","                    # Returns : (Tensor, LongTensor)\n","                    _, max_index = torch.max( outputs.data, dim = 1 )\n","                    #print( \"max_index :\", max_index )\n","\n","                    # .view(1,1) : [torch.LongTensor of size 1] → size 1×1 に reshape\n","                    action = max_index.view(1,1).to(self._device)\n","\n","            else:\n","                # ε の確率でランダムな行動を選択\n","                #action = np.random.choice( self._n_actions )\n","                action = torch.LongTensor(\n","                    [ [random.randrange(self._n_actions)] ]\n","                ).to(self._device)\n","\n","        #print( \"action :\", action )\n","        self._action_repeat = action\n","\n","        return action\n","\n","\n","    def update_q_function( self, state, action, next_state, reward ):\n","        \"\"\"\n","        Q 関数の値を更新する。\n","\n","        [Args]\n","            state : <int> 現在の状態 s のインデックス\n","            action : <int> 現在の行動 a\n","            next_state : <int> 次の状態 s'\n","            reword : <float> 報酬\n","        \n","        [Returns]\n","\n","        \"\"\"\n","        #-----------------------------------------\n","        # 経験に基づく学習用データを追加\n","        #-----------------------------------------\n","        self._memory.push( state = state, action = action, next_state = next_state, reward = reward )\n","\n","        # 学習用データがミニバッチサイズ以下ならば、以降の処理は行わない\n","        if( len(self._memory) < self._batch_size ):\n","            return\n","\n","        #-----------------------------------------        \n","        # ミニバッチデータを取得する\n","        #-----------------------------------------\n","        batch, state_batch, action_batch, reward_batch, non_final_next_states = self._memory.get_mini_batch( self._batch_size )\n","\n","        #-----------------------------------------\n","        # 教師信号となる推定行動価値関数を求める \n","        #-----------------------------------------\n","        self.predict( batch, state_batch, action_batch, reward_batch, non_final_next_states )\n","\n","        #-----------------------------------------\n","        # ネットワークを学習し、パラメーターを更新する。\n","        #-----------------------------------------\n","        self.fit()\n","\n","        return self._q_function\n","\n","\n","    def update_target_q_function( self ):\n","        \"\"\"\n","        Target Network を Main Network と同期する。\n","        \"\"\"\n","        # load_state_dict() : モデルを読み込み\n","        self._target_network.load_state_dict(\n","            state_dict = self._main_network.state_dict()    # Main Network のモデルを読み込む\n","        )\n","\n","        return"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-yr0KbdX6JPv","colab_type":"code","outputId":"d645711e-6141-4677-bff0-2dc10549dfeb","executionInfo":{"status":"error","timestamp":1553252693544,"user_tz":-540,"elapsed":257989,"user":{"displayName":"坂井祐介","photoUrl":"https://lh3.googleusercontent.com/-grt87hJXMk8/AAAAAAAAAAI/AAAAAAAACCI/DxcrthEu19k/s64/photo.jpg","userId":"18145892555689154881"}},"colab":{"base_uri":"https://localhost:8080/","height":10632}},"cell_type":"code","source":["# -*- coding:utf-8 -*-\n","# Anaconda 5.0.1 環境\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from matplotlib import animation\n","\n","# OpenAI Gym\n","import gym\n","\n","# PyTorch\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch  import nn   # ネットワークの構成関連\n","import torchvision      # 画像処理関連\n","\n","# 自作モジュール\n","#from Academy import Academy\n","#from BreakoutAcademy import BreakoutAcademy\n","#from Brain import Brain\n","#from BreakoutDQN2015Brain import BreakoutDQN2015Brain\n","#from Agent import Agent\n","#from BreakoutAgent import BreakoutAgent\n","#from ExperienceReplay import ExperienceReplay\n","#from BreakoutAtariWrappers import *\n","\n","\n","#--------------------------------\n","# 設定可能な定数\n","#--------------------------------\n","RL_ENV = \"BreakoutNoFrameskip-v0\"       # 利用する強化学習環境の課題名\n","#RL_ENV = \"Breakout-v0\"       # 利用する強化学習環境の課題名\n","NUM_EPISODE = 1000                         # エピソード試行回数(12000)\n","NUM_TIME_STEP = 5000                   # １エピソードの時間ステップの最大数\n","NUM_NOOP = 30                       # エピソード開始からの何も学習しないステップ数\n","NUM_SKIP_FRAME = 4                  # スキップするフレーム数\n","NUM_STACK_FRAME = 4                 # モデルに一度に入力する画像データのフレーム数\n","BRAIN_LEARNING_RATE = 0.0005        # 学習率(0.00025)\n","BRAIN_BATCH_SIZE = 32               # ミニバッチサイズ\n","BRAIN_GREEDY_EPSILON = 0.5          # ε-greedy 法の ε 値\n","BRAIN_GREEDY_EPSILON_INIT = 1.0         # ε-greedy 法の ε 値の初期値\n","BRAIN_GREEDY_EPSILON_FINAL = 0.1        # ε-greedy 法の ε 値の初期値\n","BRAIN_GREEDY_EPSILON_STEPS = 1000000    # ε-greedy 法の ε が減少していくフレーム数\n","BRAIN_GAMMDA = 0.99                 # 利得の割引率\n","MEMORY_CAPACITY = 50000             # Experience Relay 用の学習用データセットのメモリの最大の長さ\n","\n","\n","def main():\n","    \"\"\"\n","\t強化学習の学習環境用のブロック崩しゲーム（Breakout）\n","    ・エージェントの行動方策の学習ロジックは、DQN (2015年Natureバージョン)\n","    \"\"\"\n","    print(\"Start main()\")\n","    \n","    # バージョン確認\n","    print( \"OpenAI Gym\", gym.__version__ )\n","    print( \"PyTorch :\", torch.__version__ )\n","\n","    np.random.seed(8)\n","    random.seed(8)\n","    torch.manual_seed(8)\n","\n","    #===================================\n","    # 実行 Device の設定\n","    #===================================\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device( \"cuda\" if use_cuda else \"cpu\" )\n","    print( \"実行デバイス :\", device)\n","\n","    #===================================\n","    # 学習環境、エージェント生成フェイズ\n","    #===================================\n","    # OpenAI-Gym の ENV を作成\n","    env = make_env( \n","        device = device,\n","        env_id = RL_ENV, \n","        seed = 8,\n","        n_noop_max = NUM_NOOP,\n","        n_skip_frame = NUM_SKIP_FRAME,\n","        n_stack_frames = NUM_STACK_FRAME\n","    )\n","    \n","    print( \"env.observation_space :\", env.observation_space )\n","    print( \"env.action_space :\", env.action_space )\n","    print( \"env.unwrapped.get_action_meanings() :\", env.unwrapped.get_action_meanings() )     # 行動の値の意味\n","\n","    #-----------------------------------\n","    # Academy の生成\n","    #-----------------------------------\n","    academy = BreakoutAcademy( \n","        env = env, \n","        max_episode = NUM_EPISODE, max_time_step = NUM_TIME_STEP, \n","        save_step = 100\n","    )\n","\n","    #-----------------------------------\n","    # Brain の生成\n","    #-----------------------------------\n","    brain = BreakoutDQN2015Brain(\n","        device = device,\n","        n_states = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2], n_actions = env.action_space.n,\n","        epsilon_init = BRAIN_GREEDY_EPSILON_INIT, epsilon_final = BRAIN_GREEDY_EPSILON_FINAL, n_epsilon_step = BRAIN_GREEDY_EPSILON_STEPS,\n","        gamma = BRAIN_GAMMDA,\n","        learning_rate = BRAIN_LEARNING_RATE,\n","        batch_size = BRAIN_BATCH_SIZE,\n","        memory_capacity = MEMORY_CAPACITY,\n","        n_stack_frames = NUM_STACK_FRAME,\n","        n_skip_frames = NUM_SKIP_FRAME\n","    )\n","    \n","    # モデルの構造を定義する。\n","    #brain.model()\n","\n","    # 損失関数を設定する。\n","    #brain.loss()\n","\n","    # モデルの最適化アルゴリズムを設定\n","    brain.optimizer()\n","\n","    #-----------------------------------\n","\t# Agent の生成\n","    #-----------------------------------\n","    agent = BreakoutAgent(\n","        device = device,\n","        env = env,\n","        brain = brain,\n","        gamma = BRAIN_GAMMDA\n","    )\n","\n","    # Agent の Brain を設定\n","    agent.set_brain( brain )\n","\n","    # 学習環境に作成したエージェントを追加\n","    academy.add_agent( agent )\n","    \n","    agent.print( \"after init()\" )\n","    brain.print( \"after init()\" )\n","\n","    #===================================\n","    # エピソードの実行\n","    #===================================\n","    academy.academy_run()\n","    agent.print( \"after run\" )\n","    brain.print( \"after run\" )\n","\n","    #===================================\n","    # 学習結果の描写処理\n","    #===================================\n","    #---------------------------------------------\n","    # 利得の履歴の plot\n","    #---------------------------------------------\n","    reward_historys = agent.get_reward_historys()\n","\n","    plt.clf()\n","    plt.plot(\n","        range(0,NUM_EPISODE+1), reward_historys,\n","        label = 'gamma = {}'.format(BRAIN_GAMMDA),\n","        linestyle = '-',\n","        linewidth = 0.5,\n","        color = 'black'\n","    )\n","    plt.title( \"Reward History\" )\n","    plt.xlim( 0, NUM_EPISODE+1 )\n","    #plt.ylim( [-0.1, 1.05] )\n","    plt.xlabel( \"Episode\" )\n","    plt.grid()\n","    plt.legend( loc = \"lower right\" )\n","    plt.tight_layout()\n","\n","    plt.savefig( \n","        \"{}_DQN2015_Reward_episode{}_ts{}_lr{}_noop{}.png\".format( RL_ENV, NUM_EPISODE, NUM_TIME_STEP, BRAIN_LEARNING_RATE, NUM_NOOP ), \n","        dpi = 150, bbox_inches = \"tight\"\n","    )\n","    plt.show()\n","\n","    #-----------------------------------\n","    # 損失関数の plot\n","    #-----------------------------------\n","    loss_historys = agent.get_loss_historys()\n","\n","    plt.clf()\n","    plt.plot(\n","        range( 0, NUM_EPISODE ), loss_historys,\n","        label = 'mini_batch_size = %d, learning_rate = %0.4f' % ( BRAIN_BATCH_SIZE, BRAIN_LEARNING_RATE ),\n","        linestyle = '-',\n","        #linewidth = 2,\n","        color = 'black'\n","    )\n","    plt.title( \"loss / Smooth L1\" )\n","    plt.legend( loc = 'best' )\n","    plt.xlim( 0, NUM_EPISODE+1 )\n","    #plt.ylim( [0, 1.05] )\n","    plt.xlabel( \"Episode\" )\n","    plt.grid()\n","    plt.tight_layout()\n","    plt.savefig(\n","        \"{}_DQN2015_Loss_episode{}_ts{}_lr{}_noop{}.png\".format( RL_ENV, NUM_EPISODE, NUM_TIME_STEP, BRAIN_LEARNING_RATE, NUM_NOOP ),  \n","        dpi = 150, bbox_inches = \"tight\"\n","    )\n","    plt.show()\n","\n","    print(\"Finish main()\")\n","    return\n","\n","    \n","if __name__ == '__main__':\n","     main()\n","\n","\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Start main()\n","OpenAI Gym 0.10.11\n","PyTorch : 1.0.1.post2\n","実行デバイス : cuda\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["env.observation_space : Box(84, 84, 4)\n","env.action_space : Discrete(4)\n","env.unwrapped.get_action_meanings() : ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n","main network : QNetworkCNN(\n","  (layer): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n","    (3): ReLU()\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (5): ReLU()\n","    (6): Flatten()\n","    (7): Linear(in_features=3136, out_features=512, bias=True)\n","    (8): ReLU()\n","    (9): Linear(in_features=512, out_features=4, bias=True)\n","  )\n",")\n","target network : QNetworkCNN(\n","  (layer): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n","    (3): ReLU()\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (5): ReLU()\n","    (6): Flatten()\n","    (7): Linear(in_features=3136, out_features=512, bias=True)\n","    (8): ReLU()\n","    (9): Linear(in_features=512, out_features=4, bias=True)\n","  )\n",")\n","----------------------------------\n","BreakoutAgent\n","<__main__.BreakoutAgent object at 0x7f70e6a87668>\n","after init()\n","_device : cuda\n","_env : <WrapFrameStack<ClipRewardEnv<ScaledFloatFrame<WarpFrame<EpisodicLifeEnv<MaxAndSkipEnv<NoopResetEnv<TimeLimit<AtariEnv<Breakout-v0>>>>>>>>>>\n","_brain : \n"," <__main__.BreakoutDQN2015Brain object at 0x7f70e66e2ba8>\n","_observations : \n"," tensor([[[[0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.]]]], device='cuda:0')\n","_total_reward : \n"," tensor([0.])\n","_gamma : \n"," 0.99\n","_done : \n"," False\n","_s_a_historys : \n"," [[0, nan]]\n","_reward_historys : \n"," [0.0]\n","----------------------------------\n","----------------------------------\n","BreakoutDQN2015Brain\n","<__main__.BreakoutDQN2015Brain object at 0x7f70e66e2ba8>\n","after init()\n","_device : cuda\n","_n_states :  28224\n","_n_actions :  4\n","_epsilon :  1.0\n","_epsilon_init :  1.0\n","_epsilon_final :  0.1\n","_epsilon_step :  9.000000000000001e-07\n","_gamma :  0.99\n","_learning_rate :  0.0005\n","_batch_size :  32\n","_n_stack_frames :  4\n","_n_skip_frames :  4\n","_q_function : \n"," None\n","_expected_q_function : \n"," None\n","_memory : <__main__.ExperienceReplay object at 0x7f70e6a87588>\n","_main_network :\n"," QNetworkCNN(\n","  (layer): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n","    (3): ReLU()\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (5): ReLU()\n","    (6): Flatten()\n","    (7): Linear(in_features=3136, out_features=512, bias=True)\n","    (8): ReLU()\n","    (9): Linear(in_features=512, out_features=4, bias=True)\n","  )\n",")\n","_target_network : QNetworkCNN(\n","  (layer): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n","    (3): ReLU()\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (5): ReLU()\n","    (6): Flatten()\n","    (7): Linear(in_features=3136, out_features=512, bias=True)\n","    (8): ReLU()\n","    (9): Linear(in_features=512, out_features=4, bias=True)\n","  )\n",")\n","_loss_fn :\n"," None\n","_optimizer :\n"," RMSprop (\n","Parameter Group 0\n","    alpha: 0.99\n","    centered: False\n","    eps: 1e-08\n","    lr: 0.0005\n","    momentum: 0\n","    weight_decay: 0\n",")\n","----------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["MovieWriter imagemagick unavailable. Trying to use pillow instead.\n"],"name":"stderr"},{"output_type":"stream","text":["エピソード = 0 / 最終時間ステップ数 = 39\n","loss = 0.020283\n","エピソード = 1 / 最終時間ステップ数 = 15\n","loss = 0.023072\n","エピソード = 2 / 最終時間ステップ数 = 7\n","loss = 0.012524\n","エピソード = 3 / 最終時間ステップ数 = 33\n","loss = 0.010883\n","エピソード = 4 / 最終時間ステップ数 = 6\n","loss = 0.019647\n","エピソード = 5 / 最終時間ステップ数 = 11\n","loss = 0.018629\n","エピソード = 6 / 最終時間ステップ数 = 8\n","loss = 0.013591\n","エピソード = 7 / 最終時間ステップ数 = 6\n","loss = 0.014429\n","エピソード = 8 / 最終時間ステップ数 = 7\n","loss = 0.019595\n","エピソード = 9 / 最終時間ステップ数 = 12\n","loss = 0.009768\n","エピソード = 10 / 最終時間ステップ数 = 53\n","loss = 0.011978\n","エピソード = 11 / 最終時間ステップ数 = 8\n","loss = 0.006217\n","エピソード = 12 / 最終時間ステップ数 = 8\n","loss = 0.007844\n","エピソード = 13 / 最終時間ステップ数 = 17\n","loss = 0.005104\n","エピソード = 14 / 最終時間ステップ数 = 15\n","loss = 0.011377\n","エピソード = 15 / 最終時間ステップ数 = 72\n","loss = 0.007414\n","エピソード = 16 / 最終時間ステップ数 = 15\n","loss = 0.009901\n","エピソード = 17 / 最終時間ステップ数 = 8\n","loss = 0.029621\n","エピソード = 18 / 最終時間ステップ数 = 12\n","loss = 0.015705\n","エピソード = 19 / 最終時間ステップ数 = 15\n","loss = 0.009939\n","エピソード = 20 / 最終時間ステップ数 = 12\n","loss = 0.030119\n","エピソード = 21 / 最終時間ステップ数 = 6\n","loss = 0.037032\n","エピソード = 22 / 最終時間ステップ数 = 6\n","loss = 0.033445\n","エピソード = 23 / 最終時間ステップ数 = 6\n","loss = 0.008661\n","エピソード = 24 / 最終時間ステップ数 = 15\n","loss = 0.004582\n","エピソード = 25 / 最終時間ステップ数 = 25\n","loss = 0.007701\n","エピソード = 26 / 最終時間ステップ数 = 7\n","loss = 0.009147\n","エピソード = 27 / 最終時間ステップ数 = 28\n","loss = 0.008260\n","エピソード = 28 / 最終時間ステップ数 = 40\n","loss = 0.004378\n","エピソード = 29 / 最終時間ステップ数 = 60\n","loss = 0.003327\n","エピソード = 30 / 最終時間ステップ数 = 39\n","loss = 0.009475\n","エピソード = 31 / 最終時間ステップ数 = 32\n","loss = 0.003833\n","エピソード = 32 / 最終時間ステップ数 = 6\n","loss = 0.001819\n","エピソード = 33 / 最終時間ステップ数 = 11\n","loss = 0.008461\n","エピソード = 34 / 最終時間ステップ数 = 31\n","loss = 0.008043\n","エピソード = 35 / 最終時間ステップ数 = 12\n","loss = 0.019299\n","エピソード = 36 / 最終時間ステップ数 = 5\n","loss = 0.010810\n","エピソード = 37 / 最終時間ステップ数 = 45\n","loss = 0.013700\n","エピソード = 38 / 最終時間ステップ数 = 8\n","loss = 0.002614\n","エピソード = 39 / 最終時間ステップ数 = 6\n","loss = 0.006845\n","エピソード = 40 / 最終時間ステップ数 = 39\n","loss = 0.008698\n","エピソード = 41 / 最終時間ステップ数 = 6\n","loss = 0.009760\n","エピソード = 42 / 最終時間ステップ数 = 37\n","loss = 0.013081\n","エピソード = 43 / 最終時間ステップ数 = 17\n","loss = 0.006843\n","エピソード = 44 / 最終時間ステップ数 = 6\n","loss = 0.013267\n","エピソード = 45 / 最終時間ステップ数 = 37\n","loss = 0.012350\n","エピソード = 46 / 最終時間ステップ数 = 75\n","loss = 0.004286\n","エピソード = 47 / 最終時間ステップ数 = 19\n","loss = 0.018216\n","エピソード = 48 / 最終時間ステップ数 = 36\n","loss = 0.004866\n","エピソード = 49 / 最終時間ステップ数 = 41\n","loss = 0.002792\n","エピソード = 50 / 最終時間ステップ数 = 16\n","loss = 0.009498\n","エピソード = 51 / 最終時間ステップ数 = 31\n","loss = 0.007570\n","エピソード = 52 / 最終時間ステップ数 = 41\n","loss = 0.002055\n","エピソード = 53 / 最終時間ステップ数 = 20\n","loss = 0.003288\n","エピソード = 54 / 最終時間ステップ数 = 31\n","loss = 0.010262\n","エピソード = 55 / 最終時間ステップ数 = 21\n","loss = 0.002540\n","エピソード = 56 / 最終時間ステップ数 = 6\n","loss = 0.006146\n","エピソード = 57 / 最終時間ステップ数 = 68\n","loss = 0.005367\n","エピソード = 58 / 最終時間ステップ数 = 12\n","loss = 0.002372\n","エピソード = 59 / 最終時間ステップ数 = 44\n","loss = 0.002312\n","エピソード = 60 / 最終時間ステップ数 = 8\n","loss = 0.004585\n","エピソード = 61 / 最終時間ステップ数 = 21\n","loss = 0.004916\n","エピソード = 62 / 最終時間ステップ数 = 11\n","loss = 0.003176\n","エピソード = 63 / 最終時間ステップ数 = 28\n","loss = 0.008983\n","エピソード = 64 / 最終時間ステップ数 = 76\n","loss = 0.011839\n","エピソード = 65 / 最終時間ステップ数 = 12\n","loss = 0.003634\n","エピソード = 66 / 最終時間ステップ数 = 11\n","loss = 0.003074\n","エピソード = 67 / 最終時間ステップ数 = 7\n","loss = 0.008318\n","エピソード = 68 / 最終時間ステップ数 = 18\n","loss = 0.006115\n","エピソード = 69 / 最終時間ステップ数 = 5\n","loss = 0.013792\n","エピソード = 70 / 最終時間ステップ数 = 35\n","loss = 0.004179\n","エピソード = 71 / 最終時間ステップ数 = 15\n","loss = 0.011037\n","エピソード = 72 / 最終時間ステップ数 = 19\n","loss = 0.004393\n","エピソード = 73 / 最終時間ステップ数 = 32\n","loss = 0.013262\n","エピソード = 74 / 最終時間ステップ数 = 63\n","loss = 0.004209\n","エピソード = 75 / 最終時間ステップ数 = 29\n","loss = 0.004527\n","エピソード = 76 / 最終時間ステップ数 = 6\n","loss = 0.009022\n","エピソード = 77 / 最終時間ステップ数 = 21\n","loss = 0.004227\n","エピソード = 78 / 最終時間ステップ数 = 124\n","loss = 0.006437\n","エピソード = 79 / 最終時間ステップ数 = 63\n","loss = 0.004650\n","エピソード = 80 / 最終時間ステップ数 = 1\n","loss = 0.003369\n","エピソード = 81 / 最終時間ステップ数 = 42\n","loss = 0.007442\n","エピソード = 82 / 最終時間ステップ数 = 6\n","loss = 0.005797\n","エピソード = 83 / 最終時間ステップ数 = 6\n","loss = 0.009212\n","エピソード = 84 / 最終時間ステップ数 = 21\n","loss = 0.014964\n","エピソード = 85 / 最終時間ステップ数 = 15\n","loss = 0.012222\n","エピソード = 86 / 最終時間ステップ数 = 6\n","loss = 0.022853\n","エピソード = 87 / 最終時間ステップ数 = 6\n","loss = 0.013628\n","エピソード = 88 / 最終時間ステップ数 = 12\n","loss = 0.014581\n","エピソード = 89 / 最終時間ステップ数 = 8\n","loss = 0.015299\n","エピソード = 90 / 最終時間ステップ数 = 14\n","loss = 0.014977\n","エピソード = 91 / 最終時間ステップ数 = 6\n","loss = 0.010867\n","エピソード = 92 / 最終時間ステップ数 = 16\n","loss = 0.014252\n","エピソード = 93 / 最終時間ステップ数 = 17\n","loss = 0.017429\n","エピソード = 94 / 最終時間ステップ数 = 11\n","loss = 0.022890\n","エピソード = 95 / 最終時間ステップ数 = 7\n","loss = 0.015974\n","エピソード = 96 / 最終時間ステップ数 = 52\n","loss = 0.010253\n","エピソード = 97 / 最終時間ステップ数 = 25\n","loss = 0.021132\n"],"name":"stdout"},{"output_type":"stream","text":["MovieWriter imagemagick unavailable. Trying to use pillow instead.\n"],"name":"stderr"},{"output_type":"stream","text":["エピソード = 98 / 最終時間ステップ数 = 19\n","loss = 0.011580\n","エピソード = 99 / 最終時間ステップ数 = 8\n","loss = 0.025716\n","エピソード = 100 / 最終時間ステップ数 = 2\n","loss = 0.071037\n","エピソード = 101 / 最終時間ステップ数 = 6\n","loss = 0.019040\n","エピソード = 102 / 最終時間ステップ数 = 53\n","loss = 0.038085\n","エピソード = 103 / 最終時間ステップ数 = 8\n","loss = 0.022284\n","エピソード = 104 / 最終時間ステップ数 = 8\n","loss = 0.041056\n","エピソード = 105 / 最終時間ステップ数 = 8\n","loss = 0.021736\n","エピソード = 106 / 最終時間ステップ数 = 26\n","loss = 0.042543\n","エピソード = 107 / 最終時間ステップ数 = 21\n","loss = 0.021980\n","エピソード = 108 / 最終時間ステップ数 = 8\n","loss = 0.025009\n","エピソード = 109 / 最終時間ステップ数 = 33\n","loss = 0.060720\n","エピソード = 110 / 最終時間ステップ数 = 7\n","loss = 0.025518\n","エピソード = 111 / 最終時間ステップ数 = 6\n","loss = 0.075940\n","エピソード = 112 / 最終時間ステップ数 = 36\n","loss = 0.015457\n","エピソード = 113 / 最終時間ステップ数 = 6\n","loss = 0.018282\n","エピソード = 114 / 最終時間ステップ数 = 242\n","loss = 0.005755\n","エピソード = 115 / 最終時間ステップ数 = 11\n","loss = 0.013774\n","エピソード = 116 / 最終時間ステップ数 = 6\n","loss = 0.022494\n","エピソード = 117 / 最終時間ステップ数 = 128\n","loss = 0.013342\n","エピソード = 118 / 最終時間ステップ数 = 52\n","loss = 0.034187\n","エピソード = 119 / 最終時間ステップ数 = 6\n","loss = 0.021676\n","エピソード = 120 / 最終時間ステップ数 = 2497\n","loss = 0.007675\n","エピソード = 121 / 最終時間ステップ数 = 113\n","loss = 0.016491\n","エピソード = 122 / 最終時間ステップ数 = 2384\n","loss = 0.002990\n","エピソード = 123 / 最終時間ステップ数 = 195\n","loss = 0.002011\n","エピソード = 124 / 最終時間ステップ数 = 7\n","loss = 0.001672\n","エピソード = 125 / 最終時間ステップ数 = 5\n","loss = 0.001762\n","エピソード = 126 / 最終時間ステップ数 = 15\n","loss = 0.004219\n","エピソード = 127 / 最終時間ステップ数 = 154\n","loss = 0.010399\n","エピソード = 128 / 最終時間ステップ数 = 44\n","loss = 0.006848\n","エピソード = 129 / 最終時間ステップ数 = 332\n","loss = 0.002089\n","エピソード = 130 / 最終時間ステップ数 = 64\n","loss = 0.005123\n","エピソード = 131 / 最終時間ステップ数 = 2051\n","loss = 0.000943\n","エピソード = 132 / 最終時間ステップ数 = 1040\n","loss = 0.002000\n","エピソード = 133 / 最終時間ステップ数 = 216\n","loss = 0.005573\n","エピソード = 134 / 最終時間ステップ数 = 1232\n","loss = 0.002658\n","エピソード = 135 / 最終時間ステップ数 = 459\n","loss = 0.000641\n","エピソード = 136 / 最終時間ステップ数 = 63\n","loss = 0.004721\n","エピソード = 137 / 最終時間ステップ数 = 1966\n","loss = 0.005892\n","エピソード = 138 / 最終時間ステップ数 = 192\n","loss = 0.002200\n","エピソード = 139 / 最終時間ステップ数 = 47\n","loss = 0.001010\n","エピソード = 140 / 最終時間ステップ数 = 6\n","loss = 0.001308\n","エピソード = 141 / 最終時間ステップ数 = 196\n","loss = 0.000401\n","エピソード = 142 / 最終時間ステップ数 = 23\n","loss = 0.002302\n","エピソード = 143 / 最終時間ステップ数 = 67\n","loss = 0.006608\n","エピソード = 144 / 最終時間ステップ数 = 331\n","loss = 0.000301\n","エピソード = 145 / 最終時間ステップ数 = 123\n","loss = 0.001803\n","エピソード = 146 / 最終時間ステップ数 = 340\n","loss = 0.000841\n","エピソード = 147 / 最終時間ステップ数 = 45\n","loss = 0.011565\n","エピソード = 148 / 最終時間ステップ数 = 1\n","loss = 0.010593\n","エピソード = 149 / 最終時間ステップ数 = 1654\n","loss = 0.006674\n","エピソード = 150 / 最終時間ステップ数 = 760\n","loss = 0.004160\n","エピソード = 151 / 最終時間ステップ数 = 71\n","loss = 0.001296\n","エピソード = 152 / 最終時間ステップ数 = 112\n","loss = 0.017757\n","エピソード = 153 / 最終時間ステップ数 = 6\n","loss = 0.001636\n","エピソード = 154 / 最終時間ステップ数 = 474\n","loss = 0.004253\n","エピソード = 155 / 最終時間ステップ数 = 5\n","loss = 0.000353\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-7ae39517dea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m      \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-7ae39517dea1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# エピソードの実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m#===================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0macademy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macademy_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"after run\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"after run\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-ca1f89d7a8e0>\u001b[0m in \u001b[0;36macademy_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_step\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                     \u001b[0mdones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-5e09fd5582d5>\u001b[0m in \u001b[0;36magent_step\u001b[0;34m(self, episode, time_step)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# 行動を実行する。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m#-------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mobservations_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# numpy → PyTorch 用の型に変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-3cdcc5957e67>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mメソッドをオーバーライト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-3cdcc5957e67>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mメソッドをオーバーライト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-3cdcc5957e67>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-3cdcc5957e67>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mメソッドをオーバーライト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ale.lives\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 0 Axes>"]},"metadata":{"tags":[]}}]}]}