# DuelingNetwork による倒立振子課題（CartPole）
強化学習の学習環境用の倒立振子課題 CartPole。<br>
ディープラーニングを用いた強化学習手法である Dueling Network によって、単純な２次元の倒立振子課題を解く。<br>

※ ここでの Dueling Network のベースのネットワーク構成は、簡単のため、CNNではなく多層パーセプトロン（MLP）で代用したもので実装している。<br>

## ■ 項目 [Contents]
1. [動作環境](#動作環境)
1. [使用法](#使用法)
1. [コード説明＆実行結果](#コード説明＆実行結果)
1. 背景理論
    1. [【外部リンク】強化学習 / DuelingNetwork](http://yagami12.hatenablog.com/entry/2019/02/22/210608#DuelingNetwork)


## ■ 動作環境

- Python : 3.6
- Anaconda : 5.0.1
- OpenAIGym : 0.10.9
- PyTorch : 1.0.0

## ■ 使用法

- 使用法
```
$ python main.py
```

- 設定可能な定数
```python
[main.py]
NUM_EPISODE = 500               # エピソード試行回数
NUM_TIME_STEP = 200             # １エピソードの時間ステップの最大数
BRAIN_LEARNING_RATE = 0.0001    # 学習率
BRAIN_BATCH_SIZE = 32           # ミニバッチサイズ
BRAIN_GREEDY_EPSILON = 0.5      # ε-greedy 法の ε 値
BRAIN_GAMMDA = 0.99             # 利得の割引率
MEMORY_CAPACITY = 10000         # Experience Relay 用の学習用データセットのメモリの最大の長さ
```

<a id="コード説明＆実行結果"></a>

## ■ コード説明＆実行結果

### ◎ コードの実行結果

|パラメータ名|値（実行条件１）|
|---|---|
|エピソード試行回数：`NUM_EPISODE`|500|
|１エピソードの時間ステップの最大数：`NUM_TIME_STEP`|200|
|ミニバッチサイズ：`BRAIN_LEARNING_RATE`|32|
|学習率：`learning_rate`|0.0001|←|
|最適化アルゴリズム|Adam<br>減衰率：`beta1=0.9,beta2=0.999`|←|
|損失関数|smooth L1 関数（＝Huber 関数）|
|利得の割引率：`BRAIN_GAMMDA`|0.99|
|ε-greedy 法の ε 値の初期値：`BRAIN_GREEDY_EPSILON`|0.5（減衰）|
|Experience Relay用のメモリサイズ：`MEMORY_CAPACITY`|10000|
|報酬の設定|転倒：-1<br>連続 `NUM_TIME_STEP=200`回成功：+1<br>それ以外：0|
|シード値|`np.random.seed(8)`<br>`random.seed(8)`<br>`torch.manual_seed(8)`<br>`env.seed(8)`|
|DQNのネットワーク構成|MLP（3層）<br>入力層：状態数（4）<br>隠れ層：32ノード<br>出力層（状態価値関数）：1ノード<br>出力層（アドバンテージ関数）：行動数（2）|


- 割引利得のエピソード毎の履歴（実行条件１）<br>
![cartpole-v0_reward_episode500](https://user-images.githubusercontent.com/25688193/53857397-79015000-4019-11e9-8a39-6d95572079dc.png)<br>

- 損失関数のグラフ（実行条件１）<br>
![cartpole-v0_loss_episode500](https://user-images.githubusercontent.com/25688193/53857398-7999e680-4019-11e9-909f-c6e7b223762a.png)<br>

> DQNやDDQN より、少ないエピソード数で、学習が安定化されていることがわかる。<br>

### ◎ コードの説明


## ■ デバッグ情報

```python
[19/03/06]

adv : tensor([[ 0.0235,  0.0247],
        [ 0.0149,  0.0226],
        [ 0.0225,  0.0164],
        [ 0.0256, -0.0120],
        [ 0.0202,  0.0256],
        [ 0.0161,  0.0254],
        [ 0.0137,  0.0295],
        [ 0.0286,  0.0205],
        [ 0.0125,  0.0177],
        [ 0.0141,  0.0110],
        [ 0.0237,  0.0191],
        [ 0.0351, -0.0313],
        [ 0.0152,  0.0150],
        [ 0.0257,  0.0206],
        [ 0.0203,  0.0021],
        [ 0.0174,  0.0122],
        [ 0.0370, -0.0386],
        [ 0.0294, -0.0141],
        [ 0.0188,  0.0076],
        [ 0.0150,  0.0245],
        [ 0.0146,  0.0254],
        [ 0.0093,  0.0246],
        [ 0.0239,  0.0279],
        [ 0.0220,  0.0233],
        [ 0.0128,  0.0231],
        [ 0.0200,  0.0258],
        [ 0.0307, -0.0207],
        [ 0.0242,  0.0276],
        [ 0.0289,  0.0227]], grad_fn=<AddmmBackward>)

v_func : tensor([[-0.0253, -0.0253],
        [-0.0296, -0.0296],
        [-0.0165, -0.0165],
        [-0.0655, -0.0655],
        [-0.0069, -0.0069],
        [-0.0383, -0.0383],
        [-0.0370, -0.0370],
        [ 0.0395,  0.0395],
        [-0.0381, -0.0381],
        [-0.0368, -0.0368],
        [-0.0183, -0.0183],
        [-0.0716, -0.0716],
        [-0.0373, -0.0373],
        [-0.0268, -0.0268],
        [-0.0485, -0.0485],
        [-0.0547, -0.0547],
        [-0.0712, -0.0712],
        [-0.0730, -0.0730],
        [-0.0502, -0.0502],
        [-0.0227, -0.0227],
        [-0.0191, -0.0191],
        [-0.0311, -0.0311],
        [ 0.0114,  0.0114],
        [-0.0129, -0.0129],
        [-0.0450, -0.0450],
        [-0.0064, -0.0064],
        [-0.0694, -0.0694],
        [ 0.0108,  0.0108],
        [ 0.0030,  0.0030]], grad_fn=<ExpandBackward>)

output : tensor([[-0.0259, -0.0247],
        [-0.0335, -0.0258],
        [-0.0135, -0.0196],
        [-0.0466, -0.0843],
        [-0.0096, -0.0042],
        [-0.0429, -0.0336],
        [-0.0449, -0.0291],
        [ 0.0436,  0.0354],
        [-0.0407, -0.0355],
        [-0.0353, -0.0384],
        [-0.0160, -0.0206],
        [-0.0384, -0.1048],
        [-0.0372, -0.0374],
        [-0.0243, -0.0294],
        [-0.0394, -0.0576],
        [-0.0521, -0.0573],
        [-0.0334, -0.1090],
        [-0.0512, -0.0948],
        [-0.0446, -0.0558],
        [-0.0274, -0.0179],
        [-0.0245, -0.0137],
        [-0.0387, -0.0234],
        [ 0.0094,  0.0134],
        [-0.0135, -0.0122],
        [-0.0501, -0.0399],
        [-0.0093, -0.0035],
        [-0.0437, -0.0951],
        [ 0.0092,  0.0125],
        [ 0.0061, -0.0002]], grad_fn=<SubBackward0>)
```