# 強化学習のトイプロブレム用の迷路探索問題
> 実装中...

強化学習の学習環境用としての迷路探索問題。<br>
単純な迷路探索問題を、Unity ML-Agents のフレームワーク（`Academy`,`Brain`,`Agent`クラス など）を参考にして実装しています。<br>
分かりやすいように `main.py` ファイル毎に１つの完結した実行コードにしています。<br>

- 【参考文献＆サイト】<br>
    - [Reinforcement Learning in Unity](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design.md)<br>
    - [Unityで強化学習していたAgentのソースコードを読む話](https://ensekitt.hatenablog.com/entry/2017/12/27/123000)<br> 


## ■ 項目 [Contents]
1. [動作環境](#動作環境)
1. [コード説明＆実行結果](#コード説明＆実行結果)
    1. [等確率による迷路検索問題 : `main1.py`](#コード説明＆実行結果１)
    1. [方策勾配法による迷路検索問題 : `main2.py`](#コード説明＆実行結果２)

<!--
1. [使用するライブラリ](#使用するライブラリ)
1. [使用するデータセット](#使用するデータセット)
1. [背景理論](#背景理論)
    1. [背景理論１](#背景理論１)
    1. [](#)
-->

## ■ 動作環境

- Python : 3.6
- Anaconda : 5.0.1


<!--
<a id="使用するライブラリ"></a>

## ■ 使用するライブラリ


<a id="使用するデータセット"></a>

## ■ 使用するデータセット

-->

<a id="コード説明＆実行結果"></a>

## ■ コード説明＆実行結果

<a id="コード説明＆実行結果１"></a>

### ◎ 等確率による迷路検索問題 : `main1.py`
強化学習の学習環境用の迷路探索問題。<br>
等確率で表現された４つの移動候補（上下左右）から１つの方向を無作為に選択し、これを繰り返すことで、最終的に目的地に到着させる。<br>

![mazegame_random1](https://user-images.githubusercontent.com/25688193/49664161-67e58900-fa94-11e8-8df4-262c46446ddb.gif)<br>


<a id="コード説明＆実行結果２"></a>

### ◎ 方策勾配法による迷路検索問題 : `main2.py`
方策反復法の具体例なアルゴリズムの１つである方策勾配法によって、迷路探索問題を解く。<br>

![mazegame_policygradient1](https://user-images.githubusercontent.com/25688193/50348392-f4f00e00-057b-11e9-805a-8ee3a84b26f2.gif)<br>

```python
エピソードのステップ数： 1
迷路を解くのにかかったステップ数：23
前回の行動方針との差分： 0.018181646008
エピソードのステップ数： 2
迷路を解くのにかかったステップ数：13
前回の行動方針との差分： 0.0234385255449
エピソードのステップ数： 3
迷路を解くのにかかったステップ数：7
前回の行動方針との差分： 0.0323087713502
エピソードのステップ数： 4
迷路を解くのにかかったステップ数：7
前回の行動方針との差分： 0.0319429956863
エピソードのステップ数： 5
迷路を解くのにかかったステップ数：15
前回の行動方針との差分： 0.0185799607072
エピソードのステップ数： 6
迷路を解くのにかかったステップ数：77
前回の行動方針との差分： 0.00904734008632
エピソードのステップ数： 7
迷路を解くのにかかったステップ数：81
前回の行動方針との差分： 0.00934323007014
エピソードのステップ数： 8
迷路を解くのにかかったステップ数：97
前回の行動方針との差分： 0.00924493760322
エピソードのステップ数： 9
迷路を解くのにかかったステップ数：43
前回の行動方針との差分： 0.0104009444956
エピソードのステップ数： 10
迷路を解くのにかかったステップ数：19
前回の行動方針との差分： 0.0205719688879
...
エピソードのステップ数： 4995
迷路を解くのにかかったステップ数：5
前回の行動方針との差分： 6.85450559647e-05
エピソードのステップ数： 4996
迷路を解くのにかかったステップ数：5
前回の行動方針との差分： 6.84591462685e-05
エピソードのステップ数： 4997
迷路を解くのにかかったステップ数：5
前回の行動方針との差分： 6.83733982803e-05
エピソードのステップ数： 4998
迷路を解くのにかかったステップ数：5
前回の行動方針との差分： 6.82878115942e-05
エピソードのステップ数： 4999
迷路を解くのにかかったステップ数：5
前回の行動方針との差分： 6.82023858042e-05

最終的な policy の値
_policy : 
 [[ 0.          0.01222508  0.98777492  0.        ]
 [ 0.          0.28989193  0.          0.71010807]
 [ 0.          0.          0.40313477  0.59686523]
 [ 0.01068343  0.9813875   0.00792907  0.        ]
 [ 0.          0.          0.98764577  0.01235423]
 [ 1.          0.          0.          0.        ]
 [ 1.          0.          0.          0.        ]
 [ 0.01129659  0.98870341  0.          0.        ]]
```


---

<!--
<a id="背景理論"></a>

## 背景理論

<a id="背景理論１"></a>

## 背景理論１

-->

## デバッグメモ
```python
pi :
    array([ 0. ,  0.5,  0.5,  0. ])
    array([ 0. ,  0.5,  0. ,  0.5])
    array([ 0. ,  0. ,  0.5,  0.5])
    array([ 0.33333333,  0.33333333,  0.33333333,  0.        ])

theta :
    array([ nan,   1.,   1.,  nan])
    array([ nan,   1.,  nan,   1.])

delta_theta : N_i=38, N_ij=20, T=240
    array([        nan,  0.00416667,  1.        ,         nan])
    array([    nan,  0.0125,     nan, -0.0125])
    array([ nan,  nan,   0.,   0.])
    ...
    array([ 0.00625, -0.00625,      nan,      nan])


```